# 1. 用户行为数据准备

## 数据介绍

**页面**：主要记录一个页面的用户访问情况，包括访问时间、停留时间、页面路径等

| 字段名称       | 描述                                                         |
| -------------- | ------------------------------------------------------------ |
| page_id        | **页面id**<br />home（"首页"），category（"分类页"），discovery（"发现页"）,<br />top_n（"热门排行"），favor（"收藏页"），search（"搜索页"）,<br />good_list（"商品列表页"），good_detail（"商品详情"）， good_spec（"商品规格"）,<br />comment（"评价"），comment_done（"评价完成"），comment_list（"评价列表"）,<br />cart（"购物车"），trade（"下单结算"），payment（"支付页面"）,<br />payment_done（"支付完成"），orders_all（"全部订单"），orders_unpaid（"订单待支付"）,<br />orders_undelivered（"订单待发货"），orders_unreceipted（"订单待收货"）,<br />orders_wait_comment（"订单待评价"），mine（"我的"）,<br />activity（"活动"），login（"登录"）, register（"注册"） |
| last_page_id   | **上页id**                                                   |
| page_item_type | **页面对象类型**<br />sku_id（"商品skuId"），keyword（"搜索关键词"），sku_ids（"多个商品skuId"），<br />activity_id（"活动id"），coupon_id（"购物券id"） |
| page_item      | **页面对象id**                                               |
| sourceType     | **页面来源类型**<br />promotion（"商品推广"），recommend（"算法推荐商品"），query（"查询结果商品"），activity（"促销活动"） |
| during_time    | **停留时间**（毫秒）                                         |
| ts             | **跳入时间**                                                 |

**事件**：主要记录应用内一个具体操作行为，包括操作类型、操作对象、操作对象描述等

**曝光**：主要记录页面所曝光的内容，包括曝光对象，曝光类型等

**启动**：记录应用的启动信息

**错误**：记录应用使用过程中的错误信息，包括错误编号及错误信息

## 数据埋点

## 服务器环境搭建

+ 固定IP地址、主机名、集群映射、ssh免密登录
+ `xsync`集群分发脚本，`jpsall`查看集群进程脚本
+ JDK安装及环境变量配置

## 模拟生成日志数据

+ 将`application.yml`、`gmall2020-mock-log-2021-01-22.jar`、`path.json`、`logback.xml`上传到 **Cloud101** 的`/opt/module/applog`目录下
+ `application.yml`：**可以根据需求生成对应日期的用户行为日志**

```yml
# 外部配置打开
logging.config: "./logback.xml"
#业务日期  注意：并不是Linux系统生成日志的日期，而是生成数据中的时间
mock.date: "2022-01-01"

#模拟数据发送模式
#mock.type: "http"
#mock.type: "kafka"
mock.type: "log"

#http模式下，发送的地址
mock.url: "http://Cloud101/applog"

#kafka模式下，发送的地址
mock:
  kafka-server: "Cloud101:9092,Cloud102:9092,Cloud103:9092"
  kafka-topic: "ODS_BASE_LOG"

#启动次数
mock.startup.count: 200
#设备最大值
mock.max.mid: 500000
#会员最大值
mock.max.uid: 100
#商品最大值
mock.max.sku-id: 35
#页面平均访问时间
mock.page.during-time-ms: 20000
#错误概率 百分比
mock.error.rate: 3
#每条日志发送延迟 ms
mock.log.sleep: 10
#商品详情来源  用户查询，商品推广，智能推荐, 促销活动
mock.detail.source-type-rate: "40:25:15:20"
#领取购物券概率
mock.if_get_coupon_rate: 75
#购物券最大id
mock.max.coupon-id: 3
#搜索关键词  
mock.search.keyword: "图书,小米,iphone11,电视,口红,ps5,苹果手机,小米盒子"
```

+ `path.json`：该文件用来配置访问路径。**可以根据需求灵活配置用户点击路径**

```json
[
	{"path":["home","good_list","good_detail","cart","trade","payment"],"rate":20 },
	{"path":["home","search","good_list","good_detail","login","good_detail","cart","trade","payment"],"rate":40 },
	{"path":["home","mine","orders_unpaid","trade","payment"],"rate":10 },
	{"path":["home","mine","orders_unpaid","good_detail","good_spec","comment","trade","payment"],"rate":5 },
	{"path":["home","mine","orders_unpaid","good_detail","good_spec","comment","home"],"rate":5 },
	{"path":["home","good_detail"],"rate":10 },
	{"path":["home"  ],"rate":10 }
]
```

+ `logback.xml`：可配置日志生成路径

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property name="LOG_HOME" value="/opt/module/applog/log" />
    <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <appender name="rollingFile" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${LOG_HOME}/app.%d{yyyy-MM-dd}.log</fileNamePattern>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- 将某一个包下日志单独打印日志 -->
    <logger name="com.atgugu.gmall2020.mock.log.util.LogUtil"
            level="INFO" additivity="false">
        <appender-ref ref="rollingFile" />
        <appender-ref ref="console" />
    </logger>

    <root level="error"  >
        <appender-ref ref="console" />
    </root>
</configuration>
```

+ 生成日志：在`opt/module/applog`路径，执行以下命令

```shell
java -jar gmall2020-mock-log-2021-01-22.jar
```

### 集群日志生成脚本

+ 在 **Cloud101** 的 `/home/flash7k/bin` 下创建`lg.sh`

```sh
#!/bin/bash
for i in Cloud101 Cloud102; do
    echo "========== $i =========="
    ssh $i "cd /opt/module/applog/; java -jar gmall2020-mock-log-2021-01-22.jar >/dev/null 2>&1 &"
done 
```

> 注：
>
> + `/opt/module/applog/`为jar包及配置文件所在路径
>
> + `/dev/null`代表Linux的空设备文件，所有往这个文件里面写入的内容都会丢失，俗称“黑洞”。
>
> 标准输入0：从键盘获得输入 `/proc/self/fd/0 `
>
> 标准输出1：输出到屏幕（即控制台） `/proc/self/fd/1 `
>
> 错误输出2：输出到屏幕（即控制台） `/proc/self/fd/2`

+ 修改脚本执行权限

```shell
chmod u+x lg.sh
```

# 2. Hadoop环境

## Hadoop安装

根据情况，配置HDFS存储多目录、集群数据均衡

### 配置LZO压缩

+ 将编译好后的`hadoop-lzo-0.4.20.jar `放入`hadoop-3.1.3/share/hadoop/common/`
+ 分发`hadoop-lzo-0.4.20.jar `
+ 修改`core-site.xml`

```xml
	<!--配置支持LZO压缩-->
	<property>
        <name>io.compression.codecs</name>
        <value>
            org.apache.hadoop.io.compress.GzipCodec,
            org.apache.hadoop.io.compress.DefaultCodec,
            org.apache.hadoop.io.compress.BZip2Codec,
            org.apache.hadoop.io.compress.SnappyCodec,
            com.hadoop.compression.lzo.LzoCodec,
            com.hadoop.compression.lzo.LzopCodec
        </value>
    </property>

    <property>
        <name>io.compression.codec.lzo.class</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>
```

+ 分发`core-site.xml`

### LZO创建索引

LZO压缩文件的可切片特性依赖于其索引，故我们需要手动为LZO压缩文件创建索引。若无索引，则LZO文件的切片只有一个

```shell
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer [lzo文件路径]
```

## Hadoop调优

# 3. Zookeeper环境

## Zookeeper安装

## Zookeeper集群启停脚本

+ 在`/home/flash7k/bin`目录下创建 `zk.sh`

```sh
#!/bin/bash

case $1 in
"start"){
	for i in Cloud101 Cloud102 Cloud103
	do
        echo ---------- zookeeper $i 启动 ------------
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh start"
	done
};;
"stop"){
	for i in Cloud101 Cloud102 Cloud103
	do
        echo ---------- zookeeper $i 停止 ------------    
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"
	done
};;
"status"){
	for i in Cloud101 Cloud102 Cloud103
	do
        echo ---------- zookeeper $i 状态 ------------    
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh status"
	done
};;
esac
```

+ 增加脚本执行权限

```shell
chmod u+x zk.sh
```

# 4. Kafka环境

## Kafka安装

## Kafka集群启停脚本

+ 在`/home/flash7k/bin`目录下创建 `kafka.sh`

```sh
#! /bin/bash

case $1 in
"start"){
    for i Cloud101 Cloud102 Cloud103
    do
        echo " --------启动 $i Kafka-------"
        ssh $i "/opt/module/kafka-2.4.1/bin/kafka-server-start.sh -daemon /opt/module/kafka-2.4.1/config/server.properties"
    done
};;
"stop"){
    for i in Cloud101 Cloud102 Cloud103
    do
        echo " --------停止 $i Kafka-------"
        ssh $i "/opt/module/kafka-2.4.1/bin/kafka-server-stop.sh stop"
    done
};;
esac

```

+ 增加脚本执行权限

```shell
chmod u+x kafka.sh
```

## Kafka常用命令

+ **查看Kafka Topic列表**

```shell
bin/kafka-topics.sh --zookeeper Cloud101:2181/kafka --list
```

+ **创建Kafka Topic**

```shell
bin/kafka-topics.sh \
--zookeeper Cloud101:2181,Cloud102:2181,Cloud103:2181/kafka \
--create \
--replication-factor 1 --partitions 1 \
--topic topic_log
```

+ **删除Kafka Topic**

```shell
bin/kafka-topics.sh \
--zookeeper Cloud101:2181,Cloud102:2181,Cloud103:2181/kafka \
--delete --topic topic_log
```

+ **Kafka生产消息**

```shell
bin/kafka-console-producer.sh \
--broker-list Cloud101:9092 --topic topic_log

> hello world
> hello flash7k
```

+ **Kafka消费消息**

```shell
bin/kafka-console-consumer.sh \
--bootstrap-server Cloud101:9092 \
--from-beginning --topic topic_log
```

+ **查看Kafka Topic详情**

```shell
bin/kafka-topics.sh --zookeeper Cloud101:2181/kafka \
--describe --topic topic_log
```

## Kafka机器数量计算

**Kafka机器数量（经验公式）= 2 *（峰值生产速度 * 副本数 / 100）+ 1**

+ 峰值生产速度：压测得到

### Kafka压力测试

用Kafka官方自带的脚本，对Kafka进行压测。

+ `kafka-consumer-perf-test.sh`

+ `kafka-producer-perf-test.sh`

> Kafka压测时，在硬盘读写速度一定的情况下，可以查看到哪些地方出现了瓶颈（CPU，内存，网络IO）。一般都是网络IO达到瓶颈。 

**Kafka Producer压力测试**

+ 创建第四台服务器
+ 创建一个`test topic`，设置为3个分区2个副本

```shell
bin/kafka-topics.sh \
--zookeeper Cloud101:2181,Cloud102:2181,Cloud103:2181/kafka \
--create \
--replication-factor 2 --partitions 3 \
--topic test
```

+ 在第四台服务器上测试

```shell
bin/kafka-producer-perf-test.sh \
--topic test \
--record-size 100 --num-records 10000000 \
--throughput -1 \
--producer-props \
bootstrap.servers=Cloud101:9092,Cloud102:9092,Cloud103:9092
```

> `record-size`是一条信息有多大，单位是字节。
>
> `num-records`是总共发送多少条信息。
>
> `throughput `是每秒多少条信息，设成-1，表示不限流，尽可能快的生产数据，可测出生产者最大吞吐量。

+ **提高生产者吞吐量**
  + 调整`batch.size`
  + 调整`linger.ms`

**Kafka Consumer压力测试**

+ 如果IO，CPU，内存，网络这四个指标都不能改变，考虑增加分区数来提升性能

```shell
bin/kafka-consumer-perf-test.sh \
--broker-list Cloud101:9092,Cloud102:9092 \
--topic test \
--fetch-size 10000 --messages 10000000 --threads 1
```

> `--broker-list`指定Kafka集群地址
>
> `--topic `指定topic的名称
>
> `--fetch-size` 指定每次fetch的数据的大小
>
> `--messages` 总共要消费的消息个数

+ 提高消费者吞吐量
  + 增加`fetch-size`

## Kafka分区数计算

1. 创建一个只有1个分区的topic
2. 测试这个topic的producer吞吐量和consumer吞吐量
3. 假设他们的值分别是Tp和Tc，单位可以是MB/s。
4. 然后假设总的目标吞吐量是Tt，那么分区数 = Tt / min（Tp，Tc）

> 例如：producer吞吐量 = 20m/s；consumer吞吐量 = 50m/s，期望吞吐量100m/s
>
> 结果：分区数 = 100 / 20 = 5分区
>
> 参考：https://blog.csdn.net/weixin_42641909/article/details/89294698
>
> 通常：分区数一般设置为：3-10个

# 5. Flume采集日志

## Flume安装

## 采集组件选型

**Source**

+ `Taildir Source`相比`Exec Source`、`Spooling Directory Source`

  + `TailDir Source`：**断点续传、多目录**

    > Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传。不会丢数据，但是有可能会导致数据重复。

  + `Exec Source`：可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失

  + `Spooling Directory Source`：监控目录，支持断点续传

+ `batchSize`大小如何设置？

  + Event 1K左右时，500-1000合适（默认为100）

**Channel**

+ 采用`Kafka Channel`，省去了`Sink`，提高了效率

  > `Kafka Channel`数据存储在Kafka里面，所以数据是存储在磁盘中。

+ 注意：在Flume1.7以前，`Kafka Channel`很少有人使用，因为发现`parseAsFlumeEvent`这个配置起不了作用。也就是<u>无论`parseAsFlumeEvent`配置为true还是false，都会转为Flume Event</u>。这样的话，造成的结果是，**会始终都把Flume的headers中的信息混合着内容一起写入Kafka的消息中，而我们需要把内容写入即可**。

## 日志采集配置

Flume直接读log日志的数据，log日志的格式是`app.yyyy-mm-dd.log`，然后传输给Kafka

> **Flume配置细节自行查看官方手册**

### 创建Flume配置文件

+ 在`/opt/module/flume-1.9.0/conf`目录下创建`file-flume-kafka.conf`文件

```properties
#为各组件命名
a1.sources = r1
a1.channels = c1

#描述source
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.*
a1.sources.r1.positionFile = /opt/module/flume-1.9.0/taildir_position.json

#配置拦截器，拦截器程序自己编写并上传到集群
a1.sources.r1.interceptors =  i1
a1.sources.r1.interceptors.i1.type = com.flash7k.flume.interceptor.ETLInterceptor$Builder

#描述channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = Cloud101:9092,Cloud102:9092
a1.channels.c1.kafka.topic = topic_log
a1.channels.c1.parseAsFlumeEvent = false

#绑定source和channel以及sink和channel的关系
a1.sources.r1.channels = c1

```

> **注意：`com.flash7k.flume.interceptor.ETLInterceptor`是自定义的拦截器的全类名。需要根据用户自定义的拦截器做相应修改。**

### 自定义Flume拦截器

+ 创建Maven工程flume-interceptor
+ 创建包名：`com.flash7k.flume.interceptor`
+ 配置`pom.xml`

```xml
<dependencies>
    <!--引入flume依赖，并且设置provided，表示只是编译时使用，打包时不打入该jar包-->
    <dependency>
        <groupId>org.apache.flume</groupId>
        <artifactId>flume-ng-core</artifactId>
        <version>1.9.0</version>
        <scope>provided</scope>
    </dependency>

    <!--引入阿里巴巴json依赖，便于过滤不完整json日志-->
    <dependency>
        <groupId>com.alibaba</groupId>
        <artifactId>fastjson</artifactId>
        <version>1.2.62</version>
    </dependency>
</dependencies>

<!--引入打包插件依赖-->
<build>
    <plugins>
        <plugin>
            <artifactId>maven-compiler-plugin</artifactId>
            <version>2.3.2</version>
            <configuration>
                <source>1.8</source>
                <target>1.8</target>
            </configuration>
        </plugin>
        <plugin>
            <artifactId>maven-assembly-plugin</artifactId>
            <configuration>
                <descriptorRefs>
                    <descriptorRef>jar-with-dependencies</descriptorRef>
                </descriptorRefs>
            </configuration>
            <executions>
                <execution>
                    <id>make-assembly</id>
                    <phase>package</phase>
                    <goals>
                        <goal>single</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>

```

+ 创建`JSONUtils`类

```java
package com.flash7k.flume.interceptor;

import com.alibaba.fastjson.JSON;
import com.alibaba.fastjson.JSONException;

public class JSONUtils {
    public static boolean isJSONValidate(String log){
        try {
            //解析log，若JSON不完整则报错，没问题则返回true
            JSON.parse(log);
            return true;
        }catch (JSONException e){
            return false;
        }
    }
}
```

+ 创建`LogInterceptor`类

```java
package com.flash7k.flume.interceptor;

import com.alibaba.fastjson.JSON;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.StandardCharsets;
import java.util.Iterator;
import java.util.List;

//实现Interceptor接口，重写其中抽象方法
public class ETLInterceptor implements Interceptor {

    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {
		// 获取拦截器内容，并转化为String
        byte[] body = event.getBody();
        String log = new String(body, StandardCharsets.UTF_8);

        // 校验内容
        if (JSONUtils.isJSONValidate(log)) {
            return event;
        } else {
            return null;
        }
    }

    @Override
    public List<Event> intercept(List<Event> list) {
        // 重写多条Event
        Iterator<Event> iterator = list.iterator();

        while (iterator.hasNext()){
            Event next = iterator.next();
            if(intercept(next)==null){
                iterator.remove();
            }
        }

        return list;
    }

    public static class Builder implements Interceptor.Builder{
        @Override
        public Interceptor build() {
            return new ETLInterceptor();
        }
        @Override
        public void configure(Context context) {}
    }

    @Override
    public void close() {}
}
```

+ 打包，然后将带依赖的包传入`/opt/module/flume-1.9.0/lib`目录下
+ 分发Flume

### 测试Flume-Kafka通道

+ 分别在Cloud101、102服务器上启动Flume（最初Kafka Channel配置的是101、102）

```shell
bin/flume-ng agent --name a1 --conf-file conf/file-flume-kafka.conf &
```

+ 生成日志

```shell
lg.sh
```

+ 消费Kafka数据，观察控制台是否有数据获取到

```shell
bin/kafka-console-consumer.sh \
--bootstrap-server Cloud101:9092 \
--from-beginning --topic topic_log
```

> 说明：如果获取不到数据，先检查Kafka、Flume、Zookeeper是否都正确启动。再检查Flume的拦截器代码是否正常

## 日志采集Flume启停脚本

+ 在`/home/flash7k/bin`目录下创建脚本`flumeLog.sh`

```sh
#! /bin/bash

case $1 in
"start"){
        for i in Cloud101 Cloud102
        do
                echo " --------启动 $i 采集flume-------"
                ssh $i "nohup /opt/module/flume-1.9.0/bin/flume-ng agent --conf-file /opt/module/flume-1.9.0/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume-1.9.0/log1.txt 2>&1  &"
        done
};;	
"stop"){
        for i in Cloud101 Cloud102
        do
                echo " --------停止 $i 采集flume-------"
                ssh $i "ps -ef | grep file-flume-kafka | grep -v grep |awk  '{print \$2}' | xargs -n1 kill -9 "
        done

};;
esac
```

> 说明：
>
> + `nohup`：该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。`nohup`就是不挂起的意思，不挂断地运行命令
> + `awk`：默认分隔符为空格
> + `$2`：在“”双引号内部会被解析为脚本的第二个参数，但是这里面想表达的含义是`awk`的第二个值，所以需要将他转义，用`\$2`表示
> + `xargs `：表示取出前面命令运行的结果，作为后面命令的输入参数

+ 增加脚本执行权限

```shell
chmod u+x flumeLog.sh
```

# 6. Flume消费Kafka数据

## 消费者Flume组件选型

**Channel**

+ **FileChannel和MemoryChannel区别**
  + MemoryChannel：传输数据速度更快。但因为数据保存在JVM的堆内存中，Agent进程挂掉会导致数据丢失。适用于对数据质量要求不高的需求
  + FileChannel：传输速度相对于Memory慢，但数据安全保障高。Agent进程挂掉也可以从失败中恢复数据
  + 选型：
    + 金融类公司、对钱要求非常准确的公司通常会选择FileChannel
    + 传输普通日志信息（京东内部一天丢100万-200万条，这是非常正常的），通常选择MemoryChannel
+ **FileChannel优化**
  + 通过配置`dataDirs`指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量
  + `checkpointDir`和`backupCheckpointDir`也尽量配置在不同硬盘对应的目录中。保证`checkpoint`坏掉后，可以快速使用`backupCheckpointDir`恢复数据
  + 即传输过来的数据存储在磁盘中。在内存中创建索引，加快查询速度。备份索引到磁盘，提高可靠性。再在另一个磁盘上存储一个索引备份。

**Sink**

+ 小文件问题
  + 占用NameNode大量空间存储索引，影响性能和使用寿命
  + 每个小文件都启动Map，影响计算性能，也影响磁盘寻址时间
+ 小文件处理
  + `hdfs.rollInterval=3600`：文件创建超3600秒时会滚动生成新文件
  + `hdfs.rollSize=134217728`：文件在达到128M时会滚动生成新文件
  + `hdfs.rollCount =0`

## 消费者Flume配置

Flume直接从Kafka中读数据，上传到HDFS

### 创建Flume配置文件

+ 在 **Cloud103** 的`/opt/module/flume-1.9.0/conf`目录下创建`kafka-flume-hdfs.conf`

```properties
## 组件
a1.sources=r1
a1.channels=c1
a1.sinks=k1

## source1
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = Cloud101:9092,Cloud102:9092,Cloud103:9092
a1.sources.r1.kafka.topics=topic_log

## 配置时间戳拦截器，按照日志的上传时间存放，而不是到达时间
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.flash7k.flume.interceptor.TimeStampInterceptor$Builder

## channel1
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/flume-1.9.0/checkpoint/behavior1
a1.channels.c1.dataDirs = /opt/module/flume-1.9.0/data/behavior1/


## sink1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log-
a1.sinks.k1.hdfs.round = false

#控制生成的小文件
a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzop

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1
```

### 自定义Flume时间戳拦截器

+ **问题**：由于Flume默认会用Linux系统时间，作为输出到HDFS路径的时间。如果数据是23:59分产生的。Flume消费Kafka里面的数据时，有可能已经是第二天了，那么这部门数据会被发往第二天的HDFS路径。我们希望的是根据日志里面的实际时间，发往HDFS的路径，所以下面拦截器作用是获取日志中的实际时间

+ **解决的思路**：拦截json日志，通过fastjson框架解析json，获取实际时间ts。将获取的ts时间写入拦截器header头，header的key必须是timestamp，因为Flume框架会根据这个key的值识别为时间，写入到HDFS
+ **代码如下**

+ 在`com.flash7k.flume.interceptor`包下创建`TimeStampInterceptor`类

```java
package com.flash7k.flume.interceptor;

import com.alibaba.fastjson.JSONObject;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.StandardCharsets;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

public class TimeStampInterceptor implements Interceptor {

    private ArrayList<Event> events = new ArrayList<>();

    @Override
    public void initialize() {}

    @Override
    public Event intercept(Event event) {
		// 获取日志header和body，并转化为String
        Map<String, String> headers = event.getHeaders();
        String log = new String(event.getBody(), StandardCharsets.UTF_8);

        // 解析日志
        JSONObject jsonObject = JSONObject.parseObject(log);

        // 获取body log中的时间（即发送时间）
        String ts = jsonObject.getString("ts");
        // 修改header中的时间为发送时间
        headers.put("timestamp", ts);

        return event;
    }

    @Override
    public List<Event> intercept(List<Event> list) {
        // 创建一个ArrayList来存储header被修改的Event
        events.clear();
        for (Event event : list) {
            events.add(intercept(event));
        }

        return events;
    }

    @Override
    public void close() {}

    public static class Builder implements Interceptor.Builder {
        @Override
        public Interceptor build() {
            return new TimeStampInterceptor();
        }

        @Override
        public void configure(Context context) {}
    }
}
```

+ 重新打包，将带有依赖的包放入 **Cloud101** 的`/opt/module/flume-1.9.0/lib`目录下
+ 分发Flume

### 测试Flume-HDFS通道

## 消费者Flume启停脚本

+ 在`/home/flash7k/bin`目录下创建脚本`flumeHDFS.sh`

```sh
#! /bin/bash

case $1 in
"start"){
        for i in Cloud103
        do
                echo " --------启动 $i 消费flume-------"
                ssh $i "nohup /opt/module/flume-1.9.0/bin/flume-ng agent --conf-file /opt/module/flume-1.9.0/conf/kafka-flume-hdfs.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume-1.9.0/log2.txt 2>&1 &"
        done
};;
"stop"){
        for i in Cloud103
        do
                echo " --------停止 $i 消费flume-------"
                ssh $i "ps -ef | grep kafka-flume-hdfs | grep -v grep |awk '{print \$2}' | xargs -n1 kill"
        done

};;
esac
```

+ 增加脚本执行权限

```shell
chmod u+x flumeHDFS.sh
```

### Flume内存优化

+ 如果启动Flume出现

```shell
ERROR hdfs.HDFSEventSink: process failed
java.lang.OutOfMemoryError: GC overhead limit exceeded
```

+ 解决方案

  + 在 **Cloud101** 服务器的`/opt/module/flume-1.9.0/conf/flume-env.sh`文件中增加如下配置

  ```sh
  export JAVA_OPTS="-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote"
  ```

  + 分发配置

+ **Flume内存参数设置及优化**

  + JVM heap一般设置为4G或更高
  + `-Xmx`与`-Xms`最好设置一致，减少内存抖动带来的性能影响，如果设置不一致容易导致频繁`fullgc`
  + `-Xms`表示JVM Heap（堆内存）最小尺寸，初始分配
  + `-Xmx `表示JVM Heap（堆内存）最大允许的尺寸，按需分配
  + 如果不设置一致，容易在初始化时，由于内存不够，频繁触发`fullgc`

# 7. 日志采集通道启停脚本

+ 在`/home/flash7k/bin`目录下创建脚本`cluster.sh`

```sh
#!/bin/bash

case $1 in
"start"){
        echo ================== 启动 集群 ==================

        #启动 Zookeeper集群
        zk.sh start

        #启动 Hadoop集群
        myhadoop.sh start

        #启动 Kafka采集集群
        kafka.sh start

        #启动 Flume采集集群
        flumeLog.sh start

        #启动 Flume消费集群
        flumeHDFS.sh start

        };;
"stop"){
        echo ================== 停止 集群 ==================

        #停止 Flume消费集群
        flumeHDFS.sh stop

        #停止 Flume采集集群
        flumeLog.sh stop

        #停止 Kafka采集集群
        kafka.sh stop

        #停止 Hadoop集群
        myhadoop.sh stop

        #停止 Zookeeper集群
        zk.sh stop

};;
esac
```

